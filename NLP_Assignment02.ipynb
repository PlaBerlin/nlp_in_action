{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>目录<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1\">TF-IDF</a></span></li><li><span><a href=\"#TextRank\" data-toc-modified-id=\"TextRank-2\">TextRank</a></span></li><li><span><a href=\"#导包\" data-toc-modified-id=\"导包-3\">导包</a></span></li><li><span><a href=\"#数据准备\" data-toc-modified-id=\"数据准备-4\">数据准备</a></span></li><li><span><a href=\"#提取关键词\" data-toc-modified-id=\"提取关键词-5\">提取关键词</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF方法\" data-toc-modified-id=\"TF-IDF方法-5.1\">TF-IDF方法</a></span></li><li><span><a href=\"#实验结果\" data-toc-modified-id=\"实验结果-5.2\">实验结果</a></span></li><li><span><a href=\"#TextRank方法\" data-toc-modified-id=\"TextRank方法-5.3\">TextRank方法</a></span></li><li><span><a href=\"#实验结果\" data-toc-modified-id=\"实验结果-5.4\">实验结果</a></span></li></ul></li><li><span><a href=\"#自动摘要\" data-toc-modified-id=\"自动摘要-6\">自动摘要</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据准备\" data-toc-modified-id=\"数据准备-6.1\">数据准备</a></span></li><li><span><a href=\"#TextRank方法\" data-toc-modified-id=\"TextRank方法-6.2\">TextRank方法</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T12:30:28.109879Z",
     "start_time": "2019-12-08T12:30:27.997761Z"
    }
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "常用于评估在一个文档集中一个词对某份文档的重要程度\n",
    "\n",
    "TF:统计一个词在一篇文档中出现的频次\n",
    "\n",
    "IDF:统计一个词在文档集中的多少个文档出现，反映了这个词对文档的区分能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "核心思想来源于PageRank\n",
    "\n",
    "主要特色：脱离Corpus的支持，仅对单篇文档分析就可以提取该文档的关键词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:46:56.754724Z",
     "start_time": "2019-12-08T13:46:56.750220Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "from jieba import analyse\n",
    "import functools\n",
    "from summa import summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.668945Z",
     "start_time": "2019-12-08T13:33:23.656914Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = './assignment_data/article_exporter.json'\n",
    "with open(path, 'r', encoding='utf8', errors='ignore') as file:   \n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'stock_name': '浦发银行',\n",
       "  'title': '目前这个价格还是很合适买入持有。12.75',\n",
       "  'author': '小秘书',\n",
       "  'timing': '11-11 15:03',\n",
       "  'content': '  在12.5元至13元区间，还是值得投入长持。有一些前提一定要明确的，一是浦发的情况已经改善，还会继续完备经营管理向好。二来是市场中的浦发走向已转向右侧交易的方向，与几年前已不一样。从外资进来增持，从股市机制变化，从完备做空机制等等都有利于资金进一步加入浦发阵营。浦发今年的回归合理估值慢于好几个银行，这种情况将于现在起的一段时间会赶上。浦发的第三季业绩已公布。现阶段进入浦发，已没有几年前强弓之末时的失落。好好珍惜这时光。（从浦发明确15.05元已是最低可接受的转股价来看，向上的空间还是不少，虽然不是即时而要一些时间，但浦发已给你确立明确的信号。）2020年上证到2600点以下的观点不变，并不影响我对浦发的驻守。                                      '},\n",
       " {'stock_name': '浦发银行',\n",
       "  'title': '浦发一发完债，就跌跌不休，有点提起裤子，就不认账的赶脚[大笑]',\n",
       "  'author': 'Su999',\n",
       "  'timing': '11-11 14:05',\n",
       "  'content': '  浦发一发完债，就跌跌不休，有点提起裤子，就不认账的赶脚                                      '}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:57.923408Z",
     "start_time": "2019-12-08T13:33:57.022748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['12.5',\n",
       "  '元至',\n",
       "  '13',\n",
       "  '元',\n",
       "  '区间',\n",
       "  '值得',\n",
       "  '投入',\n",
       "  '长持',\n",
       "  '前提',\n",
       "  '一定',\n",
       "  '明确',\n",
       "  '一是',\n",
       "  '浦发',\n",
       "  '情况',\n",
       "  '已经',\n",
       "  '改善',\n",
       "  '还会',\n",
       "  '继续',\n",
       "  '完备',\n",
       "  '经营',\n",
       "  '管理',\n",
       "  '市场',\n",
       "  '中',\n",
       "  '浦发',\n",
       "  '走向',\n",
       "  '转向',\n",
       "  '右侧',\n",
       "  '交易',\n",
       "  '方向',\n",
       "  '几年',\n",
       "  '前',\n",
       "  '外资',\n",
       "  '进来',\n",
       "  '增持',\n",
       "  '股市',\n",
       "  '机制',\n",
       "  '变化',\n",
       "  '完备',\n",
       "  '做空',\n",
       "  '机制',\n",
       "  '有利于',\n",
       "  '资金',\n",
       "  '进一步',\n",
       "  '加入',\n",
       "  '浦发',\n",
       "  '阵营',\n",
       "  '浦发',\n",
       "  '今年',\n",
       "  '回归',\n",
       "  '合理',\n",
       "  '估值',\n",
       "  '慢于',\n",
       "  '好几个',\n",
       "  '银行',\n",
       "  '这种',\n",
       "  '情况',\n",
       "  '现在',\n",
       "  '一段时间',\n",
       "  '会',\n",
       "  '赶上',\n",
       "  '浦发',\n",
       "  '第三季',\n",
       "  '业绩',\n",
       "  '公布',\n",
       "  '现阶段',\n",
       "  '进入',\n",
       "  '浦发',\n",
       "  '没有',\n",
       "  '几年',\n",
       "  '前强',\n",
       "  '弓之末',\n",
       "  '时',\n",
       "  '失落',\n",
       "  '好好',\n",
       "  '珍惜',\n",
       "  '时光',\n",
       "  '浦发',\n",
       "  '明确',\n",
       "  '15.05',\n",
       "  '元',\n",
       "  '最低',\n",
       "  '接受',\n",
       "  '转',\n",
       "  '股价',\n",
       "  '来看',\n",
       "  '向上',\n",
       "  '空间',\n",
       "  '不少',\n",
       "  '即时',\n",
       "  '时间',\n",
       "  '浦发',\n",
       "  '确立',\n",
       "  '明确',\n",
       "  '信号',\n",
       "  '2020',\n",
       "  '年',\n",
       "  '上证',\n",
       "  '2600',\n",
       "  '点',\n",
       "  '以下',\n",
       "  '观点',\n",
       "  '不变',\n",
       "  '影响',\n",
       "  '浦发',\n",
       "  '驻守'],\n",
       " ['浦发', '一发', '完债', '跌跌', '不休', '有点', '提起', '裤子', '不认账', '赶脚']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stop_words(path):\n",
    "    \"\"\"从路径path中加载停止词词典\"\"\"\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "stop_words = get_stop_words('./data/stop_words.utf8')\n",
    "\n",
    "docs_list = []\n",
    "for doc in data:\n",
    "    split_words = list(jieba.cut(doc['content'].strip()))\n",
    "    split_words = [word for word in split_words if word not in stop_words and word != \" \"]\n",
    "    if split_words:\n",
    "        docs_list.append(split_words)\n",
    "docs_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "# 提取关键词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "## TF-IDF方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:34:03.410623Z",
     "start_time": "2019-12-08T13:34:03.406120Z"
    }
   },
   "outputs": [],
   "source": [
    "# idf值统计方法\n",
    "def train_idf(doc_list):\n",
    "    idf_dic = {}\n",
    "    # 总文档数\n",
    "    tt_count = len(doc_list)\n",
    "\n",
    "    # 每个词出现的文档数\n",
    "    for doc in doc_list:\n",
    "        for word in set(doc):\n",
    "            idf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "    # 按公式转换为idf值，分母加1进行平滑处理\n",
    "    for k, v in idf_dic.items():\n",
    "        idf_dic[k] = math.log(tt_count / (1.0 + v))\n",
    "\n",
    "    # 对于没有在字典中的词，默认其仅在一个文档出现，得到默认idf值\n",
    "    default_idf = math.log(tt_count / (1.0))\n",
    "    return idf_dic, default_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:34:04.290026Z",
     "start_time": "2019-12-08T13:34:04.285525Z"
    }
   },
   "outputs": [],
   "source": [
    "#  排序函数，用于topK关键词的按值排序\n",
    "def cmp(e1, e2):\n",
    "    import numpy as np\n",
    "    res = np.sign(e1[1] - e2[1])\n",
    "    if res != 0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] + e2[0]\n",
    "        b = e2[0] + e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a == b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:34:06.757532Z",
     "start_time": "2019-12-08T13:34:06.750026Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF类\n",
    "class TfIdf(object):\n",
    "    # 四个参数分别是：训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "    def __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "        self.word_list = word_list\n",
    "        self.idf_dic, self.default_idf = idf_dic, default_idf\n",
    "        self.tf_dic = self.get_tf_dic()\n",
    "        self.keyword_num = keyword_num\n",
    "\n",
    "    # 统计tf值\n",
    "    def get_tf_dic(self):\n",
    "        tf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            tf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "        tt_count = len(self.word_list)\n",
    "        for k, v in tf_dic.items():\n",
    "            tf_dic[k] = float(v) / tt_count\n",
    "\n",
    "        return tf_dic\n",
    "\n",
    "    # 按公式计算tf-idf\n",
    "    def get_tfidf(self):\n",
    "        tfidf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            idf = self.idf_dic.get(word, self.default_idf)\n",
    "            tf = self.tf_dic.get(word, 0)\n",
    "\n",
    "            tfidf = tf * idf\n",
    "            tfidf_dic[word] = tfidf\n",
    "\n",
    "        tfidf_dic.items()\n",
    "        # 根据tf-idf排序，去排名前keyword_num的词作为关键词\n",
    "        for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:34:08.734864Z",
     "start_time": "2019-12-08T13:34:08.717854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('来看', 4.328537949722896),\n",
       " ('前提', 5.02168513028284),\n",
       " ('改善', 4.7340030578310595),\n",
       " ('观点', 5.427150238391005),\n",
       " ('投入', 4.328537949722896)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计文档集docs的idf字典\n",
    "idf_dic , default_idf = train_idf(docs_list)\n",
    "list(idf_dic.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:34:17.779532Z",
     "start_time": "2019-12-08T13:34:17.773527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在12.5元至13元区间，还是值得投入长持。有一些前提一定要明确的，一是浦发的情况已经改善，还会继续完备经营管理向好。二来是市场中的浦发走向已转向右侧交易的方向，与几年前已不一样。从外资进来增持，从股市机制变化，从完备做空机制等等都有利于资金进一步加入浦发阵营。浦发今年的回归合理估值慢于好几个银行，这种情况将于现在起的一段时间会赶上。浦发的第三季业绩已公布。现阶段进入浦发，已没有几年前强弓之末时的失落。好好珍惜这时光。（从浦发明确15.05元已是最低可接受的转股价来看，向上的空间还是不少，虽然不是即时而要一些时间，但浦发已给你确立明确的信号。）2020年上证到2600点以下的观点不变，并不影响我对浦发的驻守。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单个文档doc\n",
    "doc = data[0]['content'].strip()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:34:54.026544Z",
     "start_time": "2019-12-08T13:34:54.012554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12.5', '元至', '13', '元', '区间', '值得', '投入', '长持', '前提', '一定']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对文档doc进行分词过滤\n",
    "split_words = list(jieba.cut(doc))\n",
    "doc_list = [word for word in split_words if word not in stop_words and word!=\" \"]\n",
    "doc_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:35:15.939925Z",
     "start_time": "2019-12-08T13:35:15.932920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "浦发/ 明确/ 完备/ 机制/ 几年/ 情况/ 阵营/ 长持/ 还会/ 转向/ \n"
     ]
    }
   ],
   "source": [
    "# 抽取文档前十doc的高频词\n",
    "tfidf = TfIdf(idf_dic, default_idf, doc_list, 10)\n",
    "tfidf.get_tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "## TextRank方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:37:57.471322Z",
     "start_time": "2019-12-08T13:37:57.453808Z"
    }
   },
   "outputs": [],
   "source": [
    "# 主题模型\n",
    "class TopicModel(object):\n",
    "    # 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量\n",
    "    def __init__(self, doc_list, keyword_num, model='LSI', num_topics=4):\n",
    "        # 使用gensim的接口，将文本转为向量化表示\n",
    "        # 先构建词空间\n",
    "        self.dictionary = corpora.Dictionary(doc_list)\n",
    "        # 使用BOW模型向量化\n",
    "        corpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "        # 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "        self.tfidf_model = models.TfidfModel(corpus)\n",
    "        self.corpus_tfidf = self.tfidf_model[corpus]\n",
    "\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "        # 选择加载的模型\n",
    "        if model == 'LSI':\n",
    "            self.model = self.train_lsi()\n",
    "        else:\n",
    "            self.model = self.train_lda()\n",
    "\n",
    "        # 得到数据集的主题-词分布\n",
    "        word_dic = self.word_dictionary(doc_list)\n",
    "        self.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    "\n",
    "    def train_lsi(self):\n",
    "        lsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lsi\n",
    "\n",
    "    def train_lda(self):\n",
    "        lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lda\n",
    "\n",
    "    def get_wordtopic(self, word_dic):\n",
    "        wordtopic_dic = {}\n",
    "\n",
    "        for word in word_dic:\n",
    "            single_list = [word]\n",
    "            wordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]\n",
    "            wordtopic = self.model[wordcorpus]\n",
    "            wordtopic_dic[word] = wordtopic\n",
    "        return wordtopic_dic\n",
    "\n",
    "    # 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "    def get_simword(self, word_list):\n",
    "        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "        senttopic = self.model[sentcorpus]\n",
    "\n",
    "        # 余弦相似度计算\n",
    "        def calsim(l1, l2):\n",
    "            a, b, c = 0.0, 0.0, 0.0\n",
    "            for t1, t2 in zip(l1, l2):\n",
    "                x1 = t1[1]\n",
    "                x2 = t2[1]\n",
    "                a += x1 * x1\n",
    "                b += x1 * x1\n",
    "                c += x2 * x2\n",
    "            sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "            return sim\n",
    "\n",
    "        # 计算输入文本和每个词的主题分布相似度\n",
    "        sim_dic = {}\n",
    "        for k, v in self.wordtopic_dic.items():\n",
    "            if k not in word_list:\n",
    "                continue\n",
    "            sim = calsim(v, senttopic)\n",
    "            sim_dic[k] = sim\n",
    "\n",
    "        for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n",
    "\n",
    "    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "    def word_dictionary(self, doc_list):\n",
    "        dictionary = []\n",
    "        for doc in doc_list:\n",
    "            dictionary.extend(doc)\n",
    "\n",
    "        dictionary = list(set(dictionary))\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    def doc2bowvec(self, word_list):\n",
    "        vec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "        return vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:38:23.202333Z",
     "start_time": "2019-12-08T13:38:23.197329Z"
    }
   },
   "outputs": [],
   "source": [
    "def textrank_extract(text, pos=False, keyword_num=10):\n",
    "    textrank = analyse.textrank\n",
    "    keywords = textrank(text, keyword_num)\n",
    "    # 输出抽取出的关键词\n",
    "    for keyword in keywords:\n",
    "        print(keyword + \"/ \", end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:39:24.553441Z",
     "start_time": "2019-12-08T13:39:24.508909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "机制/ 完备/ 不变/ 长持/ 走向/ 公布/ 没有/ 转向/ 进入/ 投入/ \n"
     ]
    }
   ],
   "source": [
    "textrank_extract(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "# 自动摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:33:23.690941Z",
     "start_time": "2019-12-08T13:33:23.673927Z"
    }
   },
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:47:31.953547Z",
     "start_time": "2019-12-08T13:47:31.949046Z"
    }
   },
   "outputs": [],
   "source": [
    "# 五个句子组成的文本\n",
    "text = \"\"\"Automatic summarization is the process of reducing a text document with a \\\n",
    "computer program in order to create a summary that retains the most important points \\\n",
    "of the original document. As the problem of information overload has grown, and as \\\n",
    "the quantity of data has increased, so has interest in automatic summarization. \\\n",
    "Technologies that can make a coherent summary take into account variables such as \\\n",
    "length, writing style and syntax. An example of the use of summarization technology \\\n",
    "is search engines such as Google. Document summarization is another.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T13:48:45.861048Z",
     "start_time": "2019-12-08T13:48:45.853044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选其中最重要的一个句子\n",
    "summarizer.summarize(text)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目录",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
